{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Pagination prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from itertools import islice\n",
    "import multiprocessing\n",
    "\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "from dataclasses import astuple, dataclass\n",
    "\n",
    "from urllib.parse import parse_qsl, urlsplit, unquote\n",
    "\n",
    "import parsel\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from crf import CRF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from torchnlp.encoders.text import CharacterEncoder\n",
    "from regex_delimiter_encoder import RegexDelimiterEncoder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import autopager\n",
    "# sys.path.insert(0, \"..\")\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.htmlutils import (\n",
    "    get_link_href,\n",
    "    get_link_text,\n",
    "    get_text_around_selector_list,\n",
    ")\n",
    "from autopager.parserutils import (\n",
    "    MyHTMLParser,\n",
    "    TagParser,\n",
    "    get_first_tag,\n",
    ")\n",
    "from autopager.storage import Storage\n",
    "from autopager.utils import (\n",
    "    get_domain,\n",
    "    ngrams_wb,\n",
    "    normalize,\n",
    "    replace_digits,\n",
    ")\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PPArgs:\n",
    "    batch_size: int\n",
    "    max_epochs: int\n",
    "    early_stopping_patient: int\n",
    "    precision: Literal['32-true', '16-mixed']\n",
    "    sentence_model_name: str\n",
    "    learning_rate: float\n",
    "    cls_emb_dim: int\n",
    "    cls_fc_dim: int\n",
    "    query_emb_dim: int\n",
    "    max_query_per_node: int\n",
    "    ptag_emb_dim: int\n",
    "    url_char_emb_dim: int\n",
    "    url_word_emb_dim: int\n",
    "    conv_filters: int\n",
    "    filter_sizes: int\n",
    "    url_fc_dim: int\n",
    "    lstm_hidden_dim: int\n",
    "    max_cls_query_per_node: int\n",
    "    max_url_char_tok_per_node: int\n",
    "    max_url_word_tok_per_node: int\n",
    "\n",
    "arg_parser = ArgumentParser()\n",
    "arg_parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "arg_parser.add_argument(\"--max_epochs\", type=int, default=25)\n",
    "arg_parser.add_argument(\"--early_stopping_patient\", type=int, default=20)\n",
    "arg_parser.add_argument(\"--precision\", type=str, default='32-true')\n",
    "arg_parser.add_argument(\"--sentence_model_name\", type=str, default='sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "arg_parser.add_argument(\"--learning_rate\", type=float, default=5e-4) # 0.0005\n",
    "arg_parser.add_argument(\"--cls_emb_dim\", type=int, default=32)\n",
    "arg_parser.add_argument(\"--cls_fc_dim\", type=int, default=64)\n",
    "arg_parser.add_argument(\"--query_emb_dim\", type=int, default=64)\n",
    "arg_parser.add_argument(\"--max_query_per_node\", type=int, default=32)\n",
    "arg_parser.add_argument(\"--ptag_emb_dim\", type=int, default=30)\n",
    "arg_parser.add_argument(\"--url_char_emb_dim\", type=int, default=32)\n",
    "arg_parser.add_argument(\"--url_word_emb_dim\", type=int, default=32)\n",
    "arg_parser.add_argument(\"--conv_filters\", type=int, default=64)\n",
    "arg_parser.add_argument(\"--filter_sizes\", type=json.loads, default=\"[3, 4, 5, 6]\")\n",
    "arg_parser.add_argument(\"--url_fc_dim\", type=int, default=128)\n",
    "arg_parser.add_argument(\"--lstm_hidden_dim\", type=int, default=300)\n",
    "\n",
    "arg_parser.add_argument(\"--max_cls_query_per_node\", type=int, default=256)\n",
    "arg_parser.add_argument(\"--max_url_char_tok_per_node\", type=int, default=256)\n",
    "arg_parser.add_argument(\"--max_url_word_tok_per_node\", type=int, default=128)\n",
    "\n",
    "# This may cause weird behavior!\n",
    "args = PPArgs(**vars(arg_parser.parse_args(args=[])))\n",
    "\n",
    "dict_args = vars(args)\n",
    "\n",
    "# Model candidates\n",
    "# ('sentence-transformers/paraphrase-multilingual-mpnet-base-v2') # 768 165K X\n",
    "# ('sentence-transformers/stsb-xlm-r-multilingual') # 768 51K X\n",
    "# ('sentence-transformers/distiluse-base-multilingual-cased-v2') # 512 40K ~.9\n",
    "# ('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') # 384 1.3M ~.83\n",
    "\n",
    "sentence_model = SentenceTransformer(args.sentence_model_name)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\"]\n",
    "tag2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "idx2tag = {idx: label for idx, label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_list(generator, limit=None) -> list:\n",
    "    \"\"\"\n",
    "    Generator to list with limit.\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "\n",
    "def link_to_features(link: parsel.Selector):\n",
    "    # Get text contecnt of the link otherwise alt or img.\n",
    "    # Normalize multiple white space to one and to lowercase.\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    # Retrive the line of first tag opening\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query)  # parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    # TODO: change ngrams\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(link.xpath(\".//@class\").extract())\n",
    "    parent_classes = ' '.join(link.xpath('../@class').extract())\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "\n",
    "    token_feature = {\n",
    "        'text-exact': replace_digits(text.strip()[:100].strip()),\n",
    "        # <scheme>://<netloc>/<path>?<query>#<fragment>\n",
    "        # 'url': p.path + p.query,\n",
    "        'url': href if href else \"https://\",\n",
    "        'query': query_param_names_ngrams,\n",
    "        'parent-tag': parent,\n",
    "        'class': _as_list(ngrams_wb(css_classes, 4, 5),\n",
    "                          AUTOPAGER_LIMITS.max_css_features),\n",
    "        'text': _as_list(ngrams_wb(replace_digits(text), 2, 5),\n",
    "                         AUTOPAGER_LIMITS.max_text_features),\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href == \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "    }\n",
    "    non_token_feature = []\n",
    "    for k, v in tag_feature.items():\n",
    "        if type(v) == type([]):\n",
    "            non_token_feature.extend(v)\n",
    "        else:\n",
    "            non_token_feature.append(v)\n",
    "\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    # Append sibling's text-exact to each node's text-full.\n",
    "    for feat, (before, after) in zip(feat_list, around, strict=True):\n",
    "        feat[0]['text-full'] = normalize(before) + ',' + feat[0]['text-exact'] + ',' + normalize(after)\n",
    "\n",
    "    return feat_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for idx, page in enumerate(chunks):\n",
    "        try:\n",
    "            feat_list = page_to_features(page)\n",
    "            token_features.append([node[0] for node in feat_list])\n",
    "            tag_features.append(torch.tensor([node[1] for node in feat_list]))\n",
    "        except:\n",
    "            raise Exception(f\"Error occured on {idx}\")\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_representation_with_map(tag, data_map):\n",
    "    # Vector length is the number of tags in the map(30).\n",
    "    rt_vec = [0] * len(data_map)\n",
    "    for idx, map_tag in enumerate(data_map):\n",
    "        # ('tag_name', count)\n",
    "        if tag == map_tag[0]:\n",
    "            rt_vec[idx] = 1\n",
    "            break\n",
    "    return rt_vec\n",
    "\n",
    "\n",
    "def get_ptags_vector(token_features, data_map_for_ptag: list[tuple[str, int]]):\n",
    "    pages_ptag = []\n",
    "    for page in token_features:\n",
    "        ptag_page = []\n",
    "        for node in page:\n",
    "            p_tag = node['parent-tag']\n",
    "            ptag_page.append(sparse_representation_with_map(p_tag, data_map_for_ptag))\n",
    "        pages_ptag.append(torch.tensor(ptag_page, dtype=torch.float32))\n",
    "    return pages_ptag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagTokenizer:\n",
    "    def __init__(self, tag_name_count=None):\n",
    "        rt_dict = {}\n",
    "        rt_dict[\"[PAD]\"] = 0\n",
    "        rt_dict[\"[UNK]\"] = 1\n",
    "        # TODO: Sort by count, although embedding layer should compensate this\n",
    "        if tag_name_count is not None:\n",
    "            for k in tag_name_count.keys():\n",
    "                rt_dict[k] = len(rt_dict)\n",
    "        self.map = rt_dict\n",
    "\n",
    "    def tokenize(self, word: list[str] | str):\n",
    "        if isinstance(word, list):\n",
    "            token_list = []\n",
    "            for _word in word:\n",
    "                if _word not in self.map:\n",
    "                    token_list.append(self.map[\"[UNK]\"])\n",
    "                else:\n",
    "                    token_list.append(self.map[_word])\n",
    "            return token_list\n",
    "        else:\n",
    "            if word not in self.map:\n",
    "                return self.map[\"[UNK]\"]\n",
    "            else:\n",
    "                return self.map[word]\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_word_vector(token_features) -> list[torch.Tensor]:\n",
    "    print(f\"Transform text-full to word_vector ... \")\n",
    "    # TODO: Do not use torch.tensor() to fix type hint\n",
    "    return [sentence_model.encode([node['text-full'] for node in page], convert_to_tensor=True) for page in\n",
    "            token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current test file:  ['en', 'zh', 'ko', 'ja', 'de', 'ru', 'event']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 319  domains: 109\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    rec[\"Page URL\"]\n",
    "    for rec in storage.iter_records(\n",
    "        language=None, contain_button=True, file_type=\"T\"\n",
    "    )\n",
    "]\n",
    "X_raw: list[parsel.SelectorList]\n",
    "y_raw: list[str]\n",
    "X_raw, y_raw, page_positions = storage.get_Xy(\n",
    "    language=None,\n",
    "    contain_button=True,\n",
    "    contain_position=True,\n",
    "    file_type=\"T\",\n",
    "    scaled_page=\"normal\",\n",
    ")\n",
    "print(\n",
    "    \"pages: {}  domains: {}\".format(\n",
    "        len(urls), len({get_domain(url) for url in urls})\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features: list[list[dict]]\n",
    "# x_tag: features which only have tag true/false information\n",
    "# token_features: ['text-exact', 'query', 'parent-tag', 'class', 'text', 'text-full']\n",
    "token_features, x_tag = get_token_tag_features_from_chunks(X_raw)\n",
    "token_feature_titles: list[str] = list(token_features[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_token_map = {}\n",
    "query_token_map = {}\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        for class_name in node['class']:\n",
    "            class_token_map[class_name] = class_token_map.get(class_name, 0) + 1\n",
    "        for query_name in node['query']:\n",
    "            query_token_map[query_name] = query_token_map.get(query_name, 0) + 1\n",
    "\n",
    "class_tokenizer = TagTokenizer(class_token_map)\n",
    "query_tokenizer = TagTokenizer(query_token_map)\n",
    "CLS_VOCAB_SIZE = class_tokenizer.get_size()\n",
    "QUERY_VOCAB_SIZE = query_tokenizer.get_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_query_ids(page_tokens, max_len):\n",
    "    pages_class = []\n",
    "    pages_query = []\n",
    "    for page in page_tokens:\n",
    "        class_page = []\n",
    "        query_page = []\n",
    "        for node in page:\n",
    "            # class\n",
    "            class_ids = class_tokenizer.tokenize(node['class'])\n",
    "            class_ids = class_ids + [0] * (max_len - len(class_ids))\n",
    "            class_page.append(class_ids[:max_len])\n",
    "            # query\n",
    "            query_ids = query_tokenizer.tokenize(node['query'])\n",
    "            query_ids = query_ids + [0] * (max_len - len(query_ids))\n",
    "            query_page.append(query_ids[:max_len])\n",
    "        pages_class.append(torch.tensor(class_page))\n",
    "        pages_query.append(torch.tensor(query_page))\n",
    "    return pages_class, pages_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_parent_tags = {}\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        p_tag = node['parent-tag']\n",
    "        if p_tag not in top_parent_tags:\n",
    "            top_parent_tags[p_tag] = 1\n",
    "        else:\n",
    "            top_parent_tags[p_tag] += 1\n",
    "\n",
    "sorted_parent_tags = sorted(top_parent_tags.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_full = []\n",
    "\n",
    "for page in token_features:\n",
    "    for node in page:\n",
    "        urls_full.append(node['url'])\n",
    "\n",
    "url_char_tokenizer = CharacterEncoder(urls_full)\n",
    "url_word_tokenizer = RegexDelimiterEncoder(r\"\\/|&|\\?|#|\\.|://|=|-|[\\ ]\", urls_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PAGE_X:\n",
    "    text: torch.Tensor\n",
    "    ptag: torch.Tensor\n",
    "    cls: torch.Tensor\n",
    "    query: torch.Tensor\n",
    "    url_char: torch.Tensor\n",
    "    url_word: torch.Tensor\n",
    "    tag: torch.Tensor\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(astuple(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_from_raw(x_raw, y_raw=None, token_features=None, x_tag=None) -> tuple[list[PAGE_X], list[torch.Tensor]]:\n",
    "    if token_features is None or x_tag is None:\n",
    "        token_features, x_tag = get_token_tag_features_from_chunks(x_raw)\n",
    "\n",
    "    x_text: list[torch.Tensor] = pages_to_word_vector(token_features)\n",
    "\n",
    "    x_ptag: list[torch.Tensor] = get_ptags_vector(token_features, sorted_parent_tags[slice(None, args.ptag_emb_dim)])  # type: ignore\n",
    "\n",
    "    x_class, x_query = get_class_query_ids(\n",
    "        token_features, max_len=args.max_cls_query_per_node\n",
    "    )\n",
    "\n",
    "    # (pages, nodes)\n",
    "    x_url_char_list = []\n",
    "    x_url_word_list = []\n",
    "    for page in token_features:\n",
    "        page_url_char = []\n",
    "        page_url_word = []\n",
    "        for node in page:\n",
    "            url_char = url_char_tokenizer.encode(unquote(node[\"url\"]))\n",
    "            url_char = F.pad(url_char, (0, args.max_url_char_tok_per_node - len(url_char)))\n",
    "            url_char = url_char[slice(None, args.max_url_char_tok_per_node)].long()\n",
    "            page_url_char.append(url_char)\n",
    "\n",
    "            url_word = url_word_tokenizer.encode(unquote(node[\"url\"]))\n",
    "            url_word = F.pad(url_word, (0, args.max_url_word_tok_per_node - len(url_word)))\n",
    "            url_word = url_word[slice(None, args.max_url_word_tok_per_node)].long()\n",
    "            page_url_word.append(url_word)\n",
    "        x_url_char_list.append(torch.stack(page_url_char))\n",
    "        x_url_word_list.append(torch.stack(page_url_word))\n",
    "\n",
    "    x = [PAGE_X(*x) for x in zip(x_text, x_ptag, x_class, x_query, x_url_char_list, x_url_word_list, x_tag, strict=True)]\n",
    "\n",
    "    if y_raw is not None:\n",
    "        y: list[torch.Tensor] = [\n",
    "            torch.tensor([tag2idx.get(l, 0) for l in lab]) for lab in y_raw\n",
    "        ]\n",
    "    else:\n",
    "        y = []\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_test_data(test_type=None, scaled_page='normal'):\n",
    "    if test_type is None:\n",
    "        print(\"Please assign type of test_data\")\n",
    "        return (None, None, None)\n",
    "    test_X_one = []\n",
    "    test_X_two = []\n",
    "    test_y_one = []\n",
    "    test_y_two = []\n",
    "    test_page_positions_one = []\n",
    "    test_page_positions_two = []\n",
    "    if test_type != 'EVENT_SOURCE':\n",
    "        storage.test_file = 'NORMAL'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records(exclude_en=None)]\n",
    "        test_X_one, test_y_one, test_page_positions_one = storage.get_test_Xy(validate=False, contain_position=True,\n",
    "                                                                              scaled_page=scaled_page, exclude_en=None)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if test_type == 'NORMAL':\n",
    "            return test_X_one, test_y_one, test_page_positions_one\n",
    "    if test_type != 'NORMAL':\n",
    "        storage.test_file = 'EVENT_SOURCE'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records(exclude_en=None)]\n",
    "        test_X_two, test_y_two, test_page_positions_two = storage.get_test_Xy(validate=False, contain_position=True,\n",
    "                                                                              scaled_page=scaled_page, exclude_en=None)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if test_type == 'EVENT_SOURCE':\n",
    "            return test_X_two, test_y_two, test_page_positions_two\n",
    "    test_X_raw = test_X_one + test_X_two\n",
    "    test_y = test_y_one + test_y_two\n",
    "    test_positions = test_page_positions_one + test_page_positions_two\n",
    "    return test_X_raw, test_y, test_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform text-full to word_vector ... \n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_input_from_raw(X_raw, y_raw, token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform text-full to word_vector ... \n"
     ]
    }
   ],
   "source": [
    "x_val_raw: list[parsel.selector.SelectorList]\n",
    "x_val_raw, y_val_raw = storage.get_test_Xy_by_language(language='event', contain_button=True)\n",
    "x_val, y_val = get_input_from_raw(x_val_raw, y_val_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 23  domains: 14\n",
      "Transform text-full to word_vector ... \n"
     ]
    }
   ],
   "source": [
    "test_x_raw, test_y_raw, test_page_positions = get_test_data('EVENT_SOURCE')\n",
    "x_test, y_test = get_input_from_raw(test_x_raw, test_y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(lens: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "    return torch.arange(max_len)[None, :] < lens[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPDataset(Dataset):\n",
    "    def __init__(self, x: list[PAGE_X], y: list[torch.Tensor]) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        # assert all([len(feat) == first_len for feat in self.comp_data])\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collect(batch: list[tuple]):\n",
    "    # [ (x, y), (x, y), ...]\n",
    "    xx: tuple[PAGE_X]\n",
    "    yy: tuple[torch.Tensor]\n",
    "    (xx, yy) = zip(*batch, strict=True)\n",
    "\n",
    "    # xx_pad: tuple[text, ptag, cls, query, url_char, url_word, tag]\n",
    "    xx_pad = []\n",
    "\n",
    "    # (text, ptag, cls, query, url_char, url_word, tag) = zip(*xx)\n",
    "    for feature in zip(*xx, strict=True):\n",
    "        # feature: tuple[torch.Tensor]\n",
    "        # xx_pad.append(torch.nn.utils.rnn.pack_sequence(feature))\n",
    "        xx_pad.append(pad_sequence(list(feature), batch_first=True, padding_value=0))\n",
    "\n",
    "    x_lens = [x.text.shape[0] for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    try:\n",
    "        yy_pad = pad_sequence(list(yy), batch_first=True, padding_value=0)\n",
    "    except:\n",
    "        print(f'Exception when padding y: {yy}')\n",
    "        raise\n",
    "\n",
    "    return xx_pad, yy_pad, x_lens, y_lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_x, train_y, x_val, y_val, x_test, y_test, batch_size: int = 2):\n",
    "        super().__init__()\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        return super().prepare_data()\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            # self.train_dataset, self.val_dataset = random_split(PPDataset(self.train_x, self.train_y), [0.85, 0.15])\n",
    "            self.train_dataset = PPDataset(self.train_x, self.train_y)\n",
    "            self.val_dataset = PPDataset(self.x_val, self.y_val)\n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = PPDataset(self.x_test, self.y_test)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0,\n",
    "                          collate_fn=pad_collect, drop_last=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=0,\n",
    "                          collate_fn=pad_collect, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=pad_collect)\n",
    "\n",
    "    # def teardown(self, stage: str):\n",
    "    #     # Used to clean-up when the run is finished\n",
    "    #     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPModule(pl.LightningModule):    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        learning_rate: float,\n",
    "        max_cls_query_per_node: int,\n",
    "        max_url_char_tok_per_node: int,\n",
    "        max_url_word_tok_per_node: int,\n",
    "        text_emb_dim: int,\n",
    "        cls_emb_dim: int,\n",
    "        cls_fc_dim: int,\n",
    "        query_emb_dim: int,\n",
    "        ptag_emb_dim: int,\n",
    "        max_query_per_node: int,\n",
    "        url_char_emb_dim: int,\n",
    "        url_word_emb_dim: int,\n",
    "        conv_filters: int,\n",
    "        filter_sizes: list[int],\n",
    "        url_fc_dim: int,\n",
    "        lstm_hidden_dim: int,\n",
    "        cls_vocab_size: int,\n",
    "        query_vocab_size: int,\n",
    "        url_char_vocab_size: int,\n",
    "        url_word_vocab_size: int,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams: PPArgs\n",
    "\n",
    "        # TODO: Embed hyperparameters in module\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # [BATCH, NODES, MAX_CLS_QUERY_INPUT]\n",
    "        # We use 0 to pad to 256 class/query per node\n",
    "        self.cls_emb_layer = nn.Embedding(\n",
    "            num_embeddings=self.hparams.cls_vocab_size,\n",
    "            embedding_dim=self.hparams.cls_emb_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        # [BATCH, NODES, MAX_CLS_QUERY_PER_NODE, CLS_EMB_DIM]\n",
    "        self.cls_emb_pool_layer = nn.MaxPool2d(\n",
    "            (self.hparams.max_cls_query_per_node, 1), 1, padding=(0, 0)\n",
    "        )\n",
    "\n",
    "        self.cls_conv_layer_1 = nn.Conv2d(\n",
    "            self.hparams.cls_emb_dim,\n",
    "            self.hparams.cls_emb_dim * 2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.cls_conv_layer_2 = nn.Conv2d(\n",
    "            self.hparams.cls_emb_dim,\n",
    "            self.hparams.cls_emb_dim * 2,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        # TODO: Calculate padding according to MAX_CLS_QUERY_PER_NODE and CLS_EMB_DIM\n",
    "        self.cls_pool_layer_1 = nn.MaxPool2d(3, stride=2, padding=(1, 1))\n",
    "        self.cls_pool_layer_2 = nn.MaxPool2d(3, stride=2, padding=(1, 1))\n",
    "\n",
    "        self.cls_linear_layer = nn.Linear(\n",
    "            self.hparams.max_cls_query_per_node // 2 * self.hparams.cls_emb_dim\n",
    "            + self.hparams.max_cls_query_per_node // 4 * self.hparams.cls_emb_dim\n",
    "            + self.hparams.cls_emb_dim,\n",
    "            self.hparams.cls_fc_dim,\n",
    "        )\n",
    "\n",
    "        self.ptag_linear = nn.Linear(30, 30)\n",
    "\n",
    "        self.query_emb_layer = nn.Embedding(\n",
    "            self.hparams.query_vocab_size, self.hparams.query_emb_dim, padding_idx=0\n",
    "        )\n",
    "        self.query_emb_pool_layer = nn.MaxPool2d(\n",
    "            (self.hparams.max_cls_query_per_node, 1), stride=1\n",
    "        )\n",
    "        # TODO: Add query fc layer\n",
    "\n",
    "        # in: (BATCH, #LINKS, #TOKENS) out: (BATCH, #LINKS, #TOKENS, EMBEDDING_SIZE)\n",
    "        self.url_char_emb_layer = nn.Embedding(\n",
    "            self.hparams.url_char_vocab_size,\n",
    "            self.hparams.url_char_emb_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        self.url_word_emb_layer = nn.Embedding(\n",
    "            self.hparams.url_word_vocab_size,\n",
    "            self.hparams.url_word_emb_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "        self.num_filters_total = self.hparams.conv_filters * len(self.hparams.filter_sizes)\n",
    "\n",
    "        self.url_char_convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    1,\n",
    "                    self.hparams.conv_filters,\n",
    "                    (filter_size, self.hparams.url_char_emb_dim),\n",
    "                    stride=1,\n",
    "                    padding=\"valid\",\n",
    "                    # bias=True\n",
    "                )\n",
    "                for filter_size in self.hparams.filter_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.url_char_pools = nn.ModuleList(\n",
    "            [\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=(\n",
    "                        self.hparams.max_url_char_tok_per_node - filter_size + 1,\n",
    "                        1,\n",
    "                    ),\n",
    "                    stride=(1, 1),\n",
    "                )\n",
    "                for filter_size in self.hparams.filter_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.url_char_linear_layer = nn.Linear(self.num_filters_total, 512)\n",
    "\n",
    "        self.url_word_convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(\n",
    "                    1,\n",
    "                    self.hparams.conv_filters,\n",
    "                    (filter_size, self.hparams.url_word_emb_dim),\n",
    "                    stride=1,\n",
    "                    padding=\"valid\",\n",
    "                    bias=True\n",
    "                )\n",
    "                for filter_size in self.hparams.filter_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.url_word_pools = nn.ModuleList(\n",
    "            [\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=(\n",
    "                        self.hparams.max_url_word_tok_per_node - filter_size + 1,\n",
    "                        1,\n",
    "                    ),\n",
    "                    stride=(1, 1),\n",
    "                )\n",
    "                for filter_size in self.hparams.filter_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.url_word_linear_layer = nn.Linear(self.num_filters_total, 512)\n",
    "\n",
    "        # url_char_linear_layer+url_word_linear_layer\n",
    "        self.url_linear_layer_1 = nn.Linear(1024, 512)\n",
    "        self.url_linear_layer_2 = nn.Linear(512, 256)\n",
    "        self.url_linear_layer_3 = nn.Linear(256, self.hparams.url_fc_dim)\n",
    "\n",
    "        # BiLSTM-CRF\n",
    "        # Ptag embedding: 30\n",
    "        self.embedding_dim = (\n",
    "            self.hparams.text_emb_dim\n",
    "            + self.hparams.ptag_emb_dim\n",
    "            + self.hparams.cls_fc_dim\n",
    "            + self.hparams.max_cls_query_per_node // 4 * self.hparams.cls_emb_dim\n",
    "            + self.hparams.query_emb_dim\n",
    "            + self.hparams.url_fc_dim\n",
    "            + 8\n",
    "        )\n",
    "\n",
    "        self.tag2idx = tag2idx\n",
    "        self.num_tags = len(tag2idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.embedding_dim,\n",
    "            self.hparams.lstm_hidden_dim // 2,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # O PREV PAGE NEXT\n",
    "        self.crf: CRF = CRF(tagset_size=len(tag2idx), gpu=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(self.hparams.lstm_hidden_dim, self.num_tags + 2)\n",
    "\n",
    "        # TODO: Test random or zero yield better result\n",
    "        self.hidden = self.init_hidden(2)\n",
    "\n",
    "        # self.hparams.some_layer_dim\n",
    "        self.test_predictions = []\n",
    "        self.test_label = []\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (\n",
    "            torch.randn(2, batch_size, self.hparams.lstm_hidden_dim // 2).cuda(),\n",
    "            torch.randn(2, batch_size, self.hparams.lstm_hidden_dim // 2).cuda(),\n",
    "        )\n",
    "\n",
    "    def _get_lstm_features(self, x, x_lens: list[int]):\n",
    "        x_text, x_ptag, x_class, x_query, x_url_char, x_url_word, x_tag = x\n",
    "\n",
    "        del self.hidden\n",
    "        self.hidden = self.init_hidden(x_text.shape[0])\n",
    "        # lstm_hidden_h_0 = torch.randn(2, B, LSTM_HIDDEN_DIM // 2).cuda()\n",
    "        # lstm_hidden_c_0 = torch.randn(2, B, LSTM_HIDDEN_DIM // 2).cuda()\n",
    "\n",
    "        # batch_size = x_text.shape[0]\n",
    "        # assert all(batch_size ==\n",
    "        #            feat.shape[0] for feat in x)\n",
    "\n",
    "        # x_class:\n",
    "        # (B, NODES, MAX_CLS_QUERY_PER_NODE)\n",
    "        class_emb = self.cls_emb_layer(x_class)\n",
    "        # (B, NODES, MAX_CLS_QUERY_PER_NODE, CLS_EMB_DIM)\n",
    "        class_emb_max_pool = self.cls_emb_pool_layer(class_emb)\n",
    "        # (B, NODES, 1, CLS_EMB_DIM)\n",
    "        class_emb_max_pool = class_emb_max_pool.squeeze(dim=2)\n",
    "        # (B, NODES, CLS_EMB_DIM)\n",
    "\n",
    "        # N, Cin, H, W = class_emb.shape\n",
    "\n",
    "        # class_emb.shape # (B, NODES, MAX_CLS_QUERY_PER_NODE, CLS_EMB_DIM)\n",
    "        class_emb = torch.permute(class_emb, (0, 3, 1, 2))\n",
    "        # class_emb.shape # (B, CLS_EMB_DIM, NODES, MAX_CLS_QUERY_PER_NODE)\n",
    "\n",
    "        class_conv_1 = self.cls_conv_layer_1(class_emb)\n",
    "\n",
    "        # class_conv_1.shape # (B, CLS_EMB_DIM*2, NODES, MAX_CLS_QUERY_PER_NODE)\n",
    "        class_conv_1 = torch.permute(class_conv_1, (0, 2, 3, 1))\n",
    "        # class_conv_1.shape # (B, NODES, MAX_CLS_QUERY_PER_NODE, CLS_EMB_DIM*2)\n",
    "        class_conv_1 = self.cls_pool_layer_1(class_conv_1)\n",
    "        # class_conv_1.shape # (B, NODES, MAX_CLS_QUERY_PER_NODE/2, CLS_EMB_DIM)\n",
    "\n",
    "        class_conv_2 = torch.permute(class_conv_1, (0, 3, 1, 2))\n",
    "        # class_conv_2.shape # (B, CLS_EMB_DIM, NODES, MAX_CLS_QUERY_PER_NODE/2)\n",
    "        class_conv_2 = self.cls_conv_layer_2(class_conv_2)\n",
    "        # class_conv_2.shape # (B, CLS_EMB_DIM*2, NODES, MAX_CLS_QUERY_PER_NODE/2)\n",
    "        class_conv_2 = torch.permute(class_conv_2, (0, 2, 3, 1))\n",
    "        # class_conv_2.shape # (B, NODES, MAX_CLS_QUERY_PER_NODE/2, CLS_EMB_DIM*2)\n",
    "        class_conv_2 = self.cls_pool_layer_2(class_conv_2)\n",
    "        # (B, NODES, MAX_CLS_QUERY_PER_NODE/4, CLS_EMB_DIM)\n",
    "\n",
    "        class_conv_2_flat = torch.flatten(class_conv_2, start_dim=2, end_dim=3)\n",
    "\n",
    "        class_concat = torch.cat(\n",
    "            (\n",
    "                class_emb_max_pool,\n",
    "                torch.flatten(class_conv_1, 2),\n",
    "                torch.flatten(class_conv_2, 2),\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        # class_concat.shape\n",
    "        cls_emb = self.cls_linear_layer(class_concat)\n",
    "        cls_emb = self.relu(cls_emb)\n",
    "\n",
    "        # ptag_feat = x_ptag\n",
    "        ptag_feat = self.ptag_linear(x_ptag)\n",
    "        ptag_feat = self.relu(ptag_feat)\n",
    "\n",
    "        # (B, NODES, MAX_CLS_QUERY_PER_NODE)\n",
    "        query_emb = self.query_emb_layer(x_query)\n",
    "        # (B, NODES, MAX_CLS_QUERY_PER_NODE, QUERY_EMB_DIM)\n",
    "        # (B, NODES, 256, 64)\n",
    "        query_emb = self.query_emb_pool_layer(query_emb)\n",
    "        # (B, NODES, 1, QUERY_EMB_DIM)\n",
    "        query_emb = query_emb.squeeze(2)\n",
    "\n",
    "        #############################\n",
    "\n",
    "        # (BATCH, NODES, MAX_URL_CHAR_LEN)\n",
    "        url_char_emb = self.url_char_emb_layer(x_url_char)\n",
    "        # (BATCH, NODES, MAX_URL_CHAR_LEN, URL_CHAR_EMBEDDING_SIZE)\n",
    "        url_char_emb = torch.unsqueeze(url_char_emb, 2)\n",
    "        # (BATCH, NODES, 1, MAX_URL_CHAR_LEN, URL_CHAR_EMBEDDING_SIZE)\n",
    "\n",
    "        B, F, C, H, W = url_char_emb.shape\n",
    "        url_char_emb = url_char_emb.view(-1, C, H, W)\n",
    "        # (BATCH*NODES, 1, MAX_URL_CHAR_LEN, URL_CHAR_EMBEDDING_SIZE)\n",
    "\n",
    "        pooled_char_x = []\n",
    "        for conv, pool in zip(\n",
    "            self.url_char_convs, self.url_char_pools, strict=True\n",
    "        ):\n",
    "            convolved = conv(url_char_emb)\n",
    "            # (BATCH*NODES, CONV_FILTERS, MAX_URL_CHAR_LEN-filter_size+1, 1)\n",
    "            convolved = self.relu(convolved)\n",
    "            pooled = pool(convolved)\n",
    "            # (BATCH*NODES, CONV_FILTERS, 1, 1)\n",
    "            pooled_char_x.append(pooled)\n",
    "\n",
    "        url_char_emb = torch.cat(pooled_char_x, dim=1)\n",
    "        # (BATCH*NODES, num_filters_total, 1, 1)\n",
    "\n",
    "        # Since torch.cat creates a copy we won't need it anymore\n",
    "        # TODO: Check if this prevention of memory leak work\n",
    "        del pooled_char_x\n",
    "\n",
    "        url_char_emb = torch.squeeze(url_char_emb, dim=(-1, -2))\n",
    "        # (BATCH*NODES, num_filters_total)\n",
    "\n",
    "        url_char_emb = url_char_emb.reshape(B, F, -1)\n",
    "        # (BATCH, NODES, num_filters_total)\n",
    "\n",
    "        char_output = self.url_char_linear_layer(url_char_emb)\n",
    "        # (BATCH, NODES, 512)\n",
    "        char_output = self.relu(char_output)\n",
    "\n",
    "        del B, F, C, H, W\n",
    "\n",
    "        #############################\n",
    "\n",
    "        url_word_emb = self.url_word_emb_layer(x_url_word)\n",
    "        url_word_emb = url_word_emb.unsqueeze(2)\n",
    "\n",
    "        B, F, C, H, W = url_word_emb.shape\n",
    "        url_word_emb = url_word_emb.view(-1, C, H, W)\n",
    "\n",
    "        pooled_word_x = []\n",
    "        for conv, pool in zip(\n",
    "            self.url_word_convs, self.url_word_pools, strict=True\n",
    "        ):\n",
    "            convolved = conv(url_word_emb)\n",
    "            convolved = self.relu(convolved)\n",
    "            pooled = pool(convolved)\n",
    "            pooled_word_x.append(pooled)\n",
    "\n",
    "        url_word_emb = torch.cat(pooled_word_x, dim=1)\n",
    "        del pooled_word_x\n",
    "        url_word_emb = url_word_emb.squeeze(dim=(-1, -2))\n",
    "\n",
    "        url_word_emb = url_word_emb.reshape(B, F, -1)\n",
    "\n",
    "        word_output = self.url_word_linear_layer(url_word_emb)\n",
    "        word_output = self.relu(word_output)\n",
    "\n",
    "        #############################\n",
    "\n",
    "        conv_output = torch.cat((char_output, word_output), dim=2)\n",
    "        url_emb = self.url_linear_layer_1(conv_output)\n",
    "        url_emb = self.relu(url_emb)\n",
    "        url_emb = self.url_linear_layer_2(url_emb)\n",
    "        url_emb = self.relu(url_emb)\n",
    "        url_emb = self.url_linear_layer_3(url_emb)\n",
    "        url_emb = self.relu(url_emb)\n",
    "\n",
    "        del B, F, C, H, W\n",
    "\n",
    "        #############################\n",
    "\n",
    "        # (BATCH, NODES, very_long)\n",
    "        merged = torch.cat(\n",
    "            (\n",
    "                x_text,\n",
    "                ptag_feat,\n",
    "                class_conv_2_flat,\n",
    "                cls_emb,\n",
    "                query_emb,\n",
    "                url_emb,\n",
    "                x_tag,\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "\n",
    "        # print(f\"{merged.shape}\")\n",
    "\n",
    "        packed_merged = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            merged, x_lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # By default, PyTorchâ€™s nn.LSTM module assumes the input to be sorted as [seq_len, batch_size, input_size].\n",
    "        # TODO: Find better way to get # of nodes other then merged.shape[1]\n",
    "        # merged = merged.view(merged.shape[1], batch_size, -1)\n",
    "        lstm_out, self.hidden = self.lstm(packed_merged, self.hidden)\n",
    "        # batch_first=True: NLDH (Batch_size, Sequence_length, 2, Hidden_size) (LSTM_HIDDEN_DIM//2)\n",
    "        # lstm_out = lstm_out.view(merged.shape[0], merged.shape[1], LSTM_HIDDEN_DIM)\n",
    "\n",
    "        seq_unpacked, lens_unpacked = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            lstm_out, batch_first=True, padding_value=0\n",
    "        )\n",
    "\n",
    "        lstm_feats = self.hidden2tag(seq_unpacked)\n",
    "\n",
    "        return lstm_feats\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        lstm_feats: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        x_lens: list[int],\n",
    "        y_lens: list[int],\n",
    "    ):\n",
    "        # (B, NODES, LSTM_HIDDEN_DIM)\n",
    "        # padded_lstm_feats = torch.nn.utils.rnn.pad_packed_sequence(lstm_feats, batch_first=True, padding_value=0)\n",
    "        mask: torch.Tensor = get_mask(\n",
    "            torch.Tensor(x_lens), lstm_feats.shape[1]\n",
    "        ).cuda()\n",
    "        loss = self.crf.neg_log_likelihood_loss(lstm_feats, mask, y)\n",
    "        del mask\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, x_lens: list[int]):\n",
    "        ## LSTM-CRF\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(x, x_lens)\n",
    "        # padded_lstm_feats = torch.nn.utils.rnn.pad_packed_sequence(lstm_feats, batch_first=True, padding_value=0)\n",
    "\n",
    "        # (BATCH, NODES, 300)\n",
    "        mask: torch.Tensor = get_mask(\n",
    "            torch.Tensor(x_lens), lstm_feats.shape[1]\n",
    "        ).cuda()\n",
    "\n",
    "        path_score, best_path = self.crf(lstm_feats, mask)\n",
    "        # Transition Score\n",
    "        del mask\n",
    "\n",
    "        return best_path\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, x_len, y_len = batch\n",
    "        lstm_feats = self._get_lstm_features(x, x_len)\n",
    "        loss = self.loss(lstm_feats, y, x_len, y_len)\n",
    "        self.log(\"train_loss\", loss, batch_size=x[0].shape[0], prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, x_len, y_len = batch\n",
    "        lstm_feats = self._get_lstm_features(x, x_len)\n",
    "        loss = self.loss(lstm_feats, y, x_len, y_len)\n",
    "        self.log(\"val_loss\", loss, batch_size=x[0].shape[0], prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Note: Input and result are batched\n",
    "        x, y, x_len, y_len = batch\n",
    "        lstm_feats = self._get_lstm_features(x, x_len)\n",
    "        loss = self.loss(lstm_feats, y, x_len, y_len)\n",
    "\n",
    "        best_path = self(x, x_len)\n",
    "\n",
    "        self.log(\"test_loss\", loss, batch_size=x[0].shape[0])\n",
    "\n",
    "        self.test_predictions.append(best_path)\n",
    "        self.test_label.append(y)\n",
    "        \n",
    "    def on_test_epoch_start(self) -> None:\n",
    "        del self.test_predictions\n",
    "        del self.test_label\n",
    "        self.test_predictions = []\n",
    "        self.test_label = []\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        # Flatten the list to nodes\n",
    "        test_predictions_flat = [\n",
    "            node.cpu()\n",
    "            for batch in self.test_predictions\n",
    "            for page_list in batch\n",
    "            for node in page_list\n",
    "        ]\n",
    "\n",
    "        test_label_flat = []\n",
    "        for batch in self.test_label:\n",
    "            test_label_flat.extend(batch.flatten().tolist())\n",
    "\n",
    "        reports = classification_report(\n",
    "            test_label_flat,\n",
    "            test_predictions_flat,\n",
    "            labels=[0, 1, 2, 3],\n",
    "            target_names=[\"O\", \"PREV\", \"PAGE\", \"NEXT\"],\n",
    "            digits=3,\n",
    "            output_dict=False,\n",
    "            zero_division=1,\n",
    "        )\n",
    "        print(reports)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), lr=self.hparams.learning_rate\n",
    "        )\n",
    "        return optimizer\n",
    "        # Maybe try a scheduler?\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, gamma=0.9)\n",
    "        # return [optimizer], [scheduler]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Get Class, Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_path = 'tb_logs'\n",
    "os.makedirs(logger_path, exist_ok=True)\n",
    "logger = TensorBoardLogger(logger_path, name=\"pp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14355), started 20:00:54 ago. (Use '!kill 14355' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1fc446628116f1c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1fc446628116f1c3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir $logger_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build batched crf...\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(SEED)\n",
    "\n",
    "module: PPModule = PPModule(\n",
    "    text_emb_dim=sentence_model.get_sentence_embedding_dimension(),\n",
    "    cls_vocab_size=class_tokenizer.get_size(),\n",
    "    query_vocab_size=query_tokenizer.get_size(),\n",
    "    url_char_vocab_size=url_char_tokenizer.vocab_size,\n",
    "    url_word_vocab_size=url_word_tokenizer.vocab_size,\n",
    "    **dict_args\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "dm = PPDataModule(\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test, batch_size=args.batch_size\n",
    ")\n",
    "# 'epoch' or 'step'\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# The EarlyStopping callback runs at the end of every validation epoch by default.\n",
    "# Frequency set by check_val_every_n_epoch and val_check_interval of Trainer\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00,\n",
    "    patience=args.early_stopping_patient,\n",
    "    verbose=False,\n",
    "    mode=\"min\",\n",
    "    check_finite=True,  # Stops training when loss becomes NaN or infinite\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"ckpt/\",\n",
    "    filename=\"epoch={epoch}-{step}-{val_loss:.2f}\",\n",
    "    save_top_k=5,\n",
    "    monitor=\"val_loss\",\n",
    ")\n",
    "\n",
    "# By default, Lightning logs every 50 training steps. log_every_n_steps\n",
    "trainer = pl.Trainer(\n",
    "    precision=args.precision,\n",
    "    logger=logger,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    max_epochs=args.max_epochs,\n",
    ")  # type: ignore\n",
    "# trainer = pl.Trainer(precision=PRECISION, logger=logger, callbacks=[early_stop_callback, checkpoint_callback], max_epochs=25)  # type: ignore\n",
    "print(module.hparams)\n",
    "trainer.fit(module, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: tb_logs/pp_model\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                  | Type       | Params\n",
      "------------------------------------------------------\n",
      "0  | relu                  | ReLU       | 0     \n",
      "1  | cls_emb_layer         | Embedding  | 1.1 M \n",
      "2  | cls_emb_pool_layer    | MaxPool2d  | 0     \n",
      "3  | cls_conv_layer_1      | Conv2d     | 18.5 K\n",
      "4  | cls_conv_layer_2      | Conv2d     | 51.3 K\n",
      "5  | cls_pool_layer_1      | MaxPool2d  | 0     \n",
      "6  | cls_pool_layer_2      | MaxPool2d  | 0     \n",
      "7  | cls_linear_layer      | Linear     | 395 K \n",
      "8  | ptag_linear           | Linear     | 930   \n",
      "9  | query_emb_layer       | Embedding  | 273 K \n",
      "10 | query_emb_pool_layer  | MaxPool2d  | 0     \n",
      "11 | url_char_emb_layer    | Embedding  | 7.8 K \n",
      "12 | url_word_emb_layer    | Embedding  | 1.1 M \n",
      "13 | url_char_convs        | ModuleList | 37.1 K\n",
      "14 | url_char_pools        | ModuleList | 0     \n",
      "15 | url_char_linear_layer | Linear     | 131 K \n",
      "16 | url_word_convs        | ModuleList | 37.1 K\n",
      "17 | url_word_pools        | ModuleList | 0     \n",
      "18 | url_word_linear_layer | Linear     | 131 K \n",
      "19 | url_linear_layer_1    | Linear     | 524 K \n",
      "20 | url_linear_layer_2    | Linear     | 131 K \n",
      "21 | url_linear_layer_3    | Linear     | 32.9 K\n",
      "22 | lstm                  | LSTM       | 3.6 M \n",
      "23 | crf                   | CRF        | 36    \n",
      "24 | hidden2tag            | Linear     | 1.8 K \n",
      "------------------------------------------------------\n",
      "7.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.6 M     Total params\n",
      "30.303    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefe9f8b80ac4239b6b2226fd8424594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4871206be884a0a846dff2ab05b753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1554ff83aca04da38ab3e4963f90f332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda71ea8cd034e4d8c091a27b3711d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b6bab5caf7483a8b6f1c51b51e534b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d1e536ee1c43e5912c1d17d641752f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba724887660f42c682b1a6256d16f294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f358b4d44174fb5835135cdba063a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa971a63ff14bb5a266bf63e9428414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2add94f46e7645308f1171af41accf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d744cb206e438d988ba4461b9a3e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908d9834b2ad4be29e4c77c33f286847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9329ddb7c984a9798c1ebb7dd2a1182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa95cf2828b6429d91bd5fb483d27718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f1e410b433449bb291c5b78a3f403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0656f845b84866bd96e6569589b558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5762b58ce6a2497ab21f05cc2cc20961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743003374554435d88f795432f3fbc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060ea3f56f5c4d7786a0721eeb9bfd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a83705954ec4252921db7ff37ce71ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238cbe8d102a48968c6e3b1adafb252a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7187a4ff2542088285f7c437f82799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3694e5593a9543bca2da9ef49f0a4d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4259327bfa7e4c509b9124a8c962c6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762255a1e0e2480ebba0e6e1f0485515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fd52eafd6d4c0b96aa9616a46c7744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd41c18a9af4b87b01e8a4a70858e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If need to resume training from checkpoint\n",
    "# trainer.fit(module, datamodule=dm, ckpt_path=\"/content/model_end_of_training.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(\"model_end_of_training.ckpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=epoch=10-step=1749-val_loss=11.16.ckpt\n",
      "build batched crf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63cd406eee3425d986f938cace99b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.999     1.000     0.999      5152\n",
      "        PREV      0.000     0.000     0.000         3\n",
      "        PAGE      0.962     0.781     0.862        64\n",
      "        NEXT      0.467     0.778     0.583         9\n",
      "\n",
      "    accuracy                          0.996      5228\n",
      "   macro avg      0.607     0.640     0.611      5228\n",
      "weighted avg      0.997     0.996     0.996      5228\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     2.966946840286255     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    2.966946840286255    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=epoch=18-step=3021-val_loss=10.62.ckpt\n",
      "build batched crf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d993fc420d94e5d80e8789af779a189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.997     1.000     0.998      5152\n",
      "        PREV      0.333     0.333     0.333         3\n",
      "        PAGE      0.958     0.719     0.821        64\n",
      "        NEXT      0.778     0.778     0.778         9\n",
      "\n",
      "    accuracy                          0.996      5228\n",
      "   macro avg      0.767     0.707     0.733      5228\n",
      "weighted avg      0.995     0.996     0.995      5228\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     2.887542724609375     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    2.887542724609375    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=epoch=12-step=2067-val_loss=12.25.ckpt\n",
      "build batched crf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97912a17017740f9afdd08ba8df1e8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.999     1.000     0.999      5152\n",
      "        PREV      0.000     0.000     0.000         3\n",
      "        PAGE      0.944     0.797     0.864        64\n",
      "        NEXT      0.500     0.778     0.609         9\n",
      "\n",
      "    accuracy                          0.996      5228\n",
      "   macro avg      0.611     0.644     0.618      5228\n",
      "weighted avg      0.997     0.996     0.996      5228\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     2.896695375442505     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    2.896695375442505    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=epoch=15-step=2544-val_loss=9.76.ckpt\n",
      "build batched crf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a633ec7b57d9484591bbbfbb439eb006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.997     1.000     0.999      5152\n",
      "        PREV      0.000     0.000     0.000         3\n",
      "        PAGE      0.940     0.734     0.825        64\n",
      "        NEXT      0.636     0.778     0.700         9\n",
      "\n",
      "    accuracy                          0.996      5228\n",
      "   macro avg      0.643     0.628     0.631      5228\n",
      "weighted avg      0.996     0.996     0.996      5228\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    2.7656819820404053     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m   2.7656819820404053    \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=epoch=9-step=1590-val_loss=12.12.ckpt\n",
      "build batched crf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xslin/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643f7eded60944de98800bd0d3bb0321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.998     1.000     0.999      5152\n",
      "        PREV      0.000     0.000     0.000         3\n",
      "        PAGE      0.960     0.750     0.842        64\n",
      "        NEXT      0.500     0.778     0.609         9\n",
      "\n",
      "    accuracy                          0.996      5228\n",
      "   macro avg      0.615     0.632     0.612      5228\n",
      "weighted avg      0.996     0.996     0.996      5228\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     3.26645827293396      </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35m    3.26645827293396     \u001b[0m\u001b[35m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for path in os.listdir(\"ckpt\"):\n",
    "    print(f\"{path}\")\n",
    "    module_loaded = PPModule.load_from_checkpoint(f\"ckpt/{path}\")\n",
    "    module_loaded.cuda()\n",
    "    module_loaded.eval()\n",
    "    trainer.test(module_loaded, datamodule=dm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_path: str = './autopager/data/html_all/1.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(module: PPModule, html: str) -> list[int]:\n",
    "    module.eval()\n",
    "    module.cuda()\n",
    "\n",
    "    x_raw = storage.get_single_page_X_from_html(html)\n",
    "    x, y = get_input_from_raw([x_raw])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = tuple(x[0])\n",
    "        x = [feat.unsqueeze(0).cuda() for feat in x]\n",
    "        x_len = [len(x[0][0])]\n",
    "        print(len(x[0][0]))\n",
    "        y_pred = module(x, x_lens=x_len)\n",
    "\n",
    "    return y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(module: PPModule, html: str) -> list[str]:\n",
    "    urls = []\n",
    "\n",
    "    x_raw = storage.get_single_page_X_from_html(html)\n",
    "    x, y = get_input_from_raw([x_raw])\n",
    "    y_pred = get_model_output(module, html)\n",
    "\n",
    "    for x, y in zip(x_raw, y_pred, strict=True):\n",
    "        if y == tag2idx[\"PAGE\"]:\n",
    "            urls.append(x.xpath('@href').extract_first())\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform text-full to word_vector ... \n",
      "303\n",
      "tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Transform text-full to word_vector ... \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m page_html \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(get_model_output(module_loaded, page_html))\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(get_page_links(module_loaded, page_path))\n",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m, in \u001b[0;36mget_page_links\u001b[0;34m(module, html)\u001b[0m\n\u001b[1;32m      2\u001b[0m urls \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m x_raw \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mget_single_page_X_from_html(html)\n\u001b[0;32m----> 5\u001b[0m x, y \u001b[39m=\u001b[39m get_input_from_raw([x_raw])\n\u001b[1;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m get_model_output(module, html)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(x_raw, y_pred, strict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36mget_input_from_raw\u001b[0;34m(x_raw, y_raw, token_features, x_tag)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m token_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m x_tag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     token_features, x_tag \u001b[39m=\u001b[39m get_token_tag_features_from_chunks(x_raw)\n\u001b[0;32m----> 5\u001b[0m x_text: \u001b[39mlist\u001b[39m[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m pages_to_word_vector(token_features)\n\u001b[1;32m      7\u001b[0m x_ptag: \u001b[39mlist\u001b[39m[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m get_ptags_vector(token_features, sorted_parent_tags[\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m, args\u001b[39m.\u001b[39mptag_emb_dim)])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m x_class, x_query \u001b[39m=\u001b[39m get_class_query_ids(\n\u001b[1;32m     10\u001b[0m     token_features, max_len\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmax_cls_query_per_node\n\u001b[1;32m     11\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mpages_to_word_vector\u001b[0;34m(token_features)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTransform text-full to word_vector ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# TODO: Do not use torch.tensor() to fix type hint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mreturn\u001b[39;00m [sentence_model\u001b[39m.\u001b[39mencode([node[\u001b[39m'\u001b[39m\u001b[39mtext-full\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m page], convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         token_features]\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTransform text-full to word_vector ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# TODO: Do not use torch.tensor() to fix type hint\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mreturn\u001b[39;00m [sentence_model\u001b[39m.\u001b[39;49mencode([node[\u001b[39m'\u001b[39;49m\u001b[39mtext-full\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m node \u001b[39min\u001b[39;49;00m page], convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         token_features]\n",
      "File \u001b[0;32m~/miniconda3/envs/prnsm-pt/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:195\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    192\u001b[0m all_embeddings \u001b[39m=\u001b[39m [all_embeddings[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39margsort(length_sorted_idx)]\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m convert_to_tensor:\n\u001b[0;32m--> 195\u001b[0m     all_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(all_embeddings)\n\u001b[1;32m    196\u001b[0m \u001b[39melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m    197\u001b[0m     all_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([emb\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m all_embeddings])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "with open(page_path, 'r') as f:\n",
    "    page_html = f.read()\n",
    "    print(get_model_output(module_loaded, page_html))\n",
    "    print(get_page_links(module_loaded, page_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix: str = 'pp_data/'\n",
    "try:\n",
    "    os.makedirs(prefix, exist_ok=True)\n",
    "    with open(prefix + 'url_char_tokenizer.pickle', 'wb') as f:\n",
    "        pickle.dump(url_char_tokenizer, f)\n",
    "    with open(prefix + 'url_word_tokenizer.pickle', 'wb') as f:\n",
    "        pickle.dump(url_word_tokenizer, f)\n",
    "    with open(prefix + 'class_token_map.json', 'w') as f:\n",
    "        f.write(json.dumps(class_tokenizer.map))\n",
    "    with open(prefix + 'query_token_map.json', 'w') as f:\n",
    "        f.write(json.dumps(query_tokenizer.map))\n",
    "    with open(prefix + 'sorted_parent_tags.json', 'w') as f:\n",
    "        f.write(json.dumps(sorted_parent_tags))\n",
    "except:\n",
    "    print(f\"Export fail\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug purpose codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# counter = {}\n",
    "# for obj in gc.get_objects():\n",
    "#     try:\n",
    "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "#             # print(type(obj), obj.size())\n",
    "#             if obj.size() in counter:\n",
    "#                 counter[obj.size()] += 1\n",
    "#             else:\n",
    "#                 counter[obj.size()] = 1\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# print(dict(sorted(counter.items(), key=lambda item: item[1], reverse=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PrintingCallback(pl.Callback):\n",
    "#     def on_train_start(self, trainer, pl_module):\n",
    "#         ...\n",
    "\n",
    "#     def on_train_end(self, trainer, pl_module):\n",
    "#         ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(module.model.modules())\n",
    "# print(module.model.state_dict())\n",
    "\n",
    "# from tensorboard import notebook\n",
    "# notebook.list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
